#+TITLE: Final Project - DD2412 Advanced Deep Learning

In this project, we are going to reproduce some part of the results from the paper [[Improved Autoregressive Modeling with Distribution Smoothing][https://arxiv.org/abs/2103.15089]] by Meng et al., ICLR 2021.


* TODO
*Dec 7, 2021*
Rewrite our rescaling ops
Restrict the number of epochs for all experiment to 100
- Experiment 2: focuses on the effect of smoothing
  a/ =lr = 0.0002=, initialize the weights from scratch trained on smooth data[job id: 501988]
  - After =100= epochs, we've got IS score =1.312768533148408=
  b/ =lr = 0.0002=, initialize the weights from scratch trained on clean data [job id: 501986]

- Experiment 3: focuses on the effect of using pretrained PixelCNN++ clean data
  a) Initialize model with the original weights from PixelCNN++ and train for 100 epochs on clean data [job id: 486598]
  - Generate 5 samples from this model =exp3a_baseline.png=
  - Score this model without SSD
  b) Initialize model with the original weights from PixelCNN++ and train for 100 epochs on *smooth* data and do SSD [job id: 486601]
  - Score this model with SSD

# - Experiment 1: focuses on learning rate =lr = 0.0002=, epoch 100 epochs
#  a/ =lr = 0.0002=
#  b/ =lr = 0.002=
#  c/ =lr = 0.02=

- We're investigating the differences in Inception score when doing SSD between model trained after 100 and 260 epochs.
- Next, we're gonna compute the IS on single-step denoising on the original pixelcnn++ model

*Dec 3, 2021*
- [ ] Inspect PixelCNN++

*Dec 2, 2021*
- [ ] Training Stage 1 with no. of ResNet = =5= on KTH Cluster

*Dec 1, 2021*
- [ ] Refactor `CNN_helper` for two stages of the model
- [ ] Do PixelCNN on smooth data, potentially compare with PixelCNN trained on unsmoothed data

- [X] Use MNIST, ACTUALLY USE CIFAR10
- [X] Make function to convolve an image with gaussian noise
  - [X] (NO, we don't need) Can we try this type of convonlution instead? https://scipy-lectures.org/intro/scipy/auto_examples/solutions/plot_image_blur.html

* Random thoughts
- [ ] Read about PixelCNN++. Can we use pre-trained weights?
- [ ] The amount of noise that the paper uses seems insanely high, as seen in =Autoregressive-Modeling-with-Distribution-Smoothing/configs/pixelcnnpp_conditioned_train_cifar10.yml=.
  - The images we generate with that amount of noise do not look like the presented images.

Original paper's repo: https://github.com/chenlin9/Autoregressive-Modeling-with-Distribution-Smoothing

* How to SSH
Create =~/.ssh/config=
=conda activate adlenv=
=/sshx:adlvm:/home/cuong/advdl_project/=
To copy a file: =scp -r adlvm:advdl_project/imgs/ssd.png .=
* Single step denoising
** Notes
https://github.com/Rayhane-mamah/Tacotron-2/issues/155
** Pipeline
1. Train stage 1 on smooth data. This gives $p(\tilde x)$.
2. Sample from stage 1. This returns smooth images with distribution $p(\tilde x)$, and save the images.
3. Do single step denoising

** Single step denoising
\begin{align*}
\bar x = \tilde x + \sigma^2 \nabla_{\tilde x} \log(p_\theta(\tilde x))
\end{align*}
Sampling from stage 1 network gives the image $\tilde x$
The variance $\sigma^2$ is known as a parameter.
$\nabla_{\tilde x} \log(p_\theta(\tilde x))$

* PIPELINE FOR TWO STEP DENOISING
1. Train stage 1 on smooth data. This gives $p(\tilde x)$.
2. Sample from stage 1. This returns smooth images with distribution $p(\tilde x)$, and save the images.
3. Train stage 2. This takes as input smooth images, and is trained to map those to regular images.
4. To get the final output of the network(s). Sample from stage 1 to produce a smooth image. Give that image to stage 2, and stage 2 will denoise that smooth image to produce a regular image.

*Day dreaming*
- We have an experiment coded =ssd1000= that does Stage-1 training and evaluated with SSD running on the cluster with job id =501989=